Q .6 )    YOLO MODEL

!pip install opencv-python ultralytics ipython

import cv2
from ultralytics import YOLO
from IPython.display import display, Image
import time

#load the yolo model. below is the lightweight node version.
model=YOLO('yolov8n.pt')

try:
    #open a connection to default camera
    cap=cv2.VideoCapture(0)
    if not cap.isOpened():
        raise IOError("Cannot open webcam. Please check if it is connected and not in use by other application.")

    #Set a preffered camera
    cap.set(3, 1280) #width
    cap.set(4,720)  #height

except Exception as e:
    print(f'Error: {e}')
    exit()


all_detected_objects=set()
print("Starting live detection... Press 'q' in the display window to quit")

#Main loop for detection
while True:
    #Read a frame from the camera
    success, frame=cap.read()
    if not success:
        print("Failed to grab a frame. Exiting...")
        break
    #perfrom object detection on the current frame
    results=model(frame)

    #iterated over the detected objects in the current frame
    for result in results:
        boxes=result.boxes
        for box in boxes:
            #get class id and name
            class_id=int(box.cls[0])
            class_name=model.names[class_id]
            #add the detected class naem to our master set
            all_detected_objects.add(class_name)
            #visualization
            #get confidence score and coordinates
            confidence=float(box.conf[0])
            x1, y1, x2, y2= map(int,box.xyxy[0])
            #draw the bounding box on the frame
            cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),2)
            #create the label text
            label=f'{class_name} {confidence:.2f}'
            #put the label on the bounding box
            cv2.putText(frame, label, (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0),2)

        #display the resulting frame in a window called live object detection
        cv2.imshow('Live Object Detection', frame)
        #if q is pressed, break the loop. cv2 waits 1ms for key event
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
print("\n Detection stopped by user.")

#release the camera and destroy all OpenCV windows
cap.release()
cv2.destroyAllWindows()

#print the final list of all unique objects detected during the session
if all_detected_objects:
    print("\n--- Summary of all unique objects detected---")
    #Sort all the list alphabetically for clean output
    for i,obj in enumerate(sorted(list(all_detected_objects)),1):
        print(f"{i}.{obj}")
    print('----------------------------------------------------')
else:
    print("\nNo objects were detected during the session")




Q.7) AUTOENCODERS

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from sklearn.manifold import TSNE

(x_train,y_train),(x_test,y_test)=mnist.load_data()
x_train=x_train.astype('float32')/255
x_test=x_test.astype('float32')/255
x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))
x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))

#build autoencoder model
#this is size of encoded representations
encoding_dim=32
#This is input placeholder
input_img=Input(shape=(784,))
#'encoded' representation of the input
encoded=Dense(encoding_dim,activation='relu')(input_img)

#decoded is the lossy reconstruction of the input
decoded=Dense(784,activation='sigmoid')(encoded)
#this model maps an input to its representations
autoencoder=Model(input_img,decoded)
#this model maps an input to its encoded representation
encoder=Model(input_img, decoded)

#creating a placeholder
encoded_input=Input(shape=(encoding_dim,))
#retrieve last layer of autoencoder model
decoder_layer=autoencoder.layers[-1]
#Create the decoder model
decoder=Model(encoded_input, decoder_layer(encoded_input))

#compile the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

#Train the autoencoder

history=autoencoder.fit(x_train, x_train,
                       epochs=50,
                       batch_size=256,
                       shuffle=True,
                       validation_data=(x_test,x_test))

decoded_imgs=autoencoder.predict(x_test)
n=10 #how many digits you want to display
plt.figure(figsize=(20,4))
for i in range(n):
    ax=plt.subplot(2,n,i+1)
    plt.imshow(x_test[i].reshape(28,28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    if i==0:
        ax.set_title("Original Images", loc='left')
    ax=plt.subplot(2,n,i+1+n)
    plt.imshow(decoded_imgs[i].reshape(28,28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    if i==0:
        ax.set_title("Reconstructed Images", loc='left')
plt.show()

#plotting and training the validation loss
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'],label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss during training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

#visualize the latent space using tsne

encoded_imgs=encoder.predict(x_test)
#latent space to 2D
tsne=TSNE(n_components=2,random_state=42)
encoded_imgs_2d=tsne.fit_transform(encoded_imgs)

plt.figure(figsize=(12,10))
plt.scatter(encoded_imgs_2d[:,0], encoded_imgs_2d[:,1], c=y_test, cmap='jet', s=10)
plt.colorbar()
plt.title('t-SNE visualization of the MNIST latent space')
plt.xlabel('t-SNE dimension 1')
plt.ylabel('t-SNE dimension 2')
plt.show()




Q.8 ) RNN FOR SEQUENCE PREDICTION

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import yfinance as yf

stock_data=yf.download('AAPL', start='2012-01-01',end='2022-01-01')
df=stock_data[['Close']]

df['Close']=df['Close'].rolling(window=5).mean()
df.dropna(inplace=True)

scaler=MinMaxScaler(feature_range=(0,1))
scaled_data=scaler.fit_transform(df)

def create_dataset(data,time_step=1):
  X,Y=[],[]
  for i in range(len(data)-time_step-1):
    X.append(data[i:(i+time_step),0])
    Y.append(data[i+time_step,0])
  return np.array(X), np.array(Y)

time_step=60
X,Y=create_dataset(scaled_data,time_step)

train_size=int(len(X) * 0.8)
X_train,X_test=X[:train_size],X[train_size:]
Y_train, Y_test=Y[:train_size],Y[train_size:]

X_train=X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)

#building and training lstm model
model=Sequential()
model.add(LSTM(50,return_sequences=True, input_shape=(time_step,1)))
model.add(Dropout(0.2))
model.add(LSTM(50,return_sequences=True))
model.add(Dropout(0.2))
model.add(Dense(25))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

#add early stopping
early_stop=EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history=model.fit(X_train,Y_train,epochs=100,batch_size=32, validation_data=(X_test,Y_test),callbacks=[early_stop], verbose=1)

train_predict=model.predict(X_train)
test_predict=model.predict(X_test)

#inverse transformation to get actual values
train_predict=scaler.inverse_transform(train_predict[:,:,0])
test_predict=scaler.inverse_transform(test_predict[:,:,0])
Y_train=scaler.inverse_transform(Y_train.reshape(-1,1))
Y_test=scaler.inverse_transform(Y_test.reshape(-1,1))

#calculate rmse
train_rmse=np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))
test_rmse=np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))
print(f'Train RMSE: {train_rmse}')
print(f'Test RMSE: {test_rmse}')

#future predictions
n_future=100
last_sequence=scaled_data[-time_step:]
predictions=[]

for i in range(n_future):
  next_prediction = model.predict(last_sequence.reshape(1, time_step, 1), verbose=0)  # disable progress bar
  next_value = next_prediction[0, 0, 0]  # scalar value
  predictions.append(next_value)

    # Reshape to (1, 1) to match the shape of each step in last_sequence
  next_value_reshaped = np.array([[next_value]])

    # Now last_sequence[1:] is shape (time_step - 1, 1), and next_value_reshaped is (1, 1)
  last_sequence = np.concatenate((last_sequence[1:], next_value_reshaped), axis=0)

predictions=np.array(predictions)
future_predictions=scaler.inverse_transform(predictions.reshape(-1,1))

#Visualizations
#Lstm predictions vs actual values
plt.figure(figsize=(12,6))
plt.plot(df.index,df['Close'], label='Actual', color='blue')
plt.plot(df.index[time_step:time_step +len(train_predict)], train_predict, color='green', label='Train Predictions')

plt.plot(df.index[-len(test_predict):], test_predict, color='red', label='Test Predictions')

#Plot the future predictions
future_dates=pd.date_range(df.index[-1], periods=n_future, freq='D')
plt.plot(future_dates, future_predictions, color='orange', label='Future Predictions')

plt.title('LSTM predictions vs actual')
plt.xlabel('time')
plt.ylabel('Close Price')
plt.legend()
plt.show()




Q. 9) RNN FOR SEQUENCE PREDICTION (LSTM)

# LSTM Implementation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import yfinance as yf

# 1. Load the Dataset
stock_data = yf.download('AAPL', start='2012-01-01', end='2022-01-01')
df = stock_data[['Close']]

# Apply a moving average to smooth the data
df['Close'] = df['Close'].rolling(window=5).mean()
df.dropna(inplace=True)  # Drop NaN values created by the moving average

# 2. Data Preprocessing
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df)
# Create dataset for LSTM
def create_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data)-time_step-1):
        X.append(data[i:(i+time_step), 0])
        Y.append(data[i + time_step, 0])
    return np.array(X), np.array(Y)
    
# Prepare the dataset with a time step of 60 (60 days of data)
time_step = 60
X, Y = create_dataset(scaled_data, time_step)

# Split the data into training and test sets
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
Y_train, Y_test = Y[:train_size], Y[train_size:]

# Reshape the input to be [samples, time steps, features] for LSTM
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# 3. Build and Train the LSTM Model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))
model.add(Dropout(0.2))  # Adding dropout for regularization
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))  # Adding another dropout layer
model.add(Dense(25))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Add early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), callbacks=[early_stop], verbose=1)

# 4. Prediction and Evaluation
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

# Inverse transform to get the actual values
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
Y_train = scaler.inverse_transform([Y_train])
Y_test = scaler.inverse_transform([Y_test])

# Calculate RMSE
train_rmse = np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))
test_rmse = np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))

print(f'Train RMSE: {train_rmse}')
print(f'Test RMSE: {test_rmse}')

# 5. Future Predictions using LSTM
n_future = 100  # Predicting 30 days into the future
last_sequence = scaled_data[-time_step:]
predictions = []

for _ in range(n_future):
    next_prediction = model.predict(last_sequence.reshape(1, time_step, 1))
    predictions.append(next_prediction[0, 0])
    last_sequence = np.append(last_sequence[1:], next_prediction, axis=0)
predictions = np.array(predictions)
future_predictions = scaler.inverse_transform(predictions.reshape(-1, 1))

# 6. Visualizations
# LSTM Predictions vs. Actual Values
plt.figure(figsize=(12,6))
# Plot the actual data
plt.plot(df.index, df['Close'], label='Actual', color='blue')

# Plot the training predictions
plt.plot(df.index[time_step:time_step + len(train_predict)], train_predict, color='green', label='Train Predictions')

# Plot the testing predictions
plt.plot(df.index[-len(test_predict):], test_predict, color='red', label='Test Predictions')

# Plot the future predictions
future_dates = pd.date_range(df.index[-1], periods=n_future, freq='D')
plt.plot(future_dates, future_predictions, color='orange', label='LSTM Future Predictions')

plt.legend()
plt.title('LSTM Predictions vs. Actual')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.show()




Q. 10 ) GAN

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

#check if gpu is available
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device:{device}')

#load mnist dataset and preprocess the data
transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5],[0.5])  #normalize images to [-1,1]
])
train_dataset=torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=256, shuffle=True)

#define generator model
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.model=nn.Sequential(
            nn.Linear(100,7*7*256),
            nn.BatchNorm1d(7*7*256),
            nn.LeakyReLU(0.2),
            nn.Unflatten(1,(256,7,7)),
            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(128,64, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2),
            nn.ConvTranspose2d(64,1, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.Tanh()
        )
    def forward(self,x):
        return self.model(x)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model=nn.Sequential(
            nn.Conv2d(1,64,kernel_size=5, stride=2, padding=2),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Conv2d(64,128,kernel_size=5,stride=2,padding=2),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Flatten(),
            nn.Linear(128*7*7,1),
            nn.Sigmoid()
        )
    def forward(self,x):
        return self.model(x)

#initialize models and move them to GPU if available
generator=Generator().to(device)
discriminator=Discriminator().to(device)

#define loss function and optimizers
criterion=nn.BCELoss()
generator_optimizer=optim.Adam(generator.parameters(), lr=1e-4)
discriminator_optimizer=optim.Adam(discriminator.parameters(), lr=1e-4)

#function to generate random noise
def generate_noise(batch_size, noise_dim):
    return torch.randn(batch_size, noise_dim,device=device)

#training step for 1 batch
def train_step(real_images):
    batch_size=real_images.size(0)
    #train discriminator
    noise=generate_noise(batch_size,100)
    fake_images=generator(noise)
    real_labels=torch.ones(batch_size,1, device=device)
    fake_labels=torch.zeros(batch_size,1,device=device)
    real_images=real_images.to(device)

    #discriminator loss on real and fake images
    real_output=discriminator(real_images)
    fake_output=discriminator(fake_images.detach())
    real_loss=criterion(real_output,real_labels)
    fake_loss=criterion(fake_output,fake_labels)
    discriminator_loss=real_loss+fake_loss
    discriminator_optimizer.zero_grad()
    discriminator_loss.backward()
    discriminator_optimizer.step()

    #train generator
    noise=generate_noise(batch_size,100)
    fake_images=generator(noise)
    fake_output=discriminator(fake_images)
    generator_loss=criterion(fake_output,real_labels)
    generator_optimizer.zero_grad()
    generator_loss.backward()
    generator_optimizer.step()
    return discriminator_loss.item(), generator_loss.item()

#generate and save images
def generate_and_save_images(epoch,fixed_noise):
    fake_images=generator(fixed_noise).cpu().detach()
    fake_images=(fake_images+1)/2.0
    fig,axes=plt.subplots(4,4,figsize=(4,4))
    for i, ax in enumerate(axes.flatten()):
        ax.imshow(fake_images[i,0,:,:],cmap='gray')
        ax.axis('off')
    plt.savefig(f'images at epoch{epoch:04d}.png')
    plt.show()

#training loop
EPOCHS=100
noise_dim=100
num_examples_to_generate=16
fixed_noise=generate_noise(num_examples_to_generate,noise_dim)

for epoch in range(EPOCHS):
    total_d_loss=0
    total_g_loss=0
    for real_images, _ in train_loader:
        d_loss,g_loss=train_step(real_images)
        total_d_loss+=d_loss
        total_g_loss +=g_loss
    #print losses and generate images
    print(f'Epoch {epoch+1}/{EPOCHS} | DLoss: {total_d_loss/len(train_loader):.4f} | {total_g_loss / len(train_loader):.4f}')
    if (epoch+1)%10==0:
        generate_and_save_images(epoch+1, fixed_noise)




Q . 11) AI IMAGE GENERATION USING TEXT PROMPTS

!pip install diffusers
!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
!pip install transformers
!pip install pillow
!pip install accelerate
!pip install safetensors

!pip install hf_xet
!pip install --upgrade ipywidgets

from diffusers import StableDiffusionPipeline
import torch

model=StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4").to("cuda")
prompt="A beautiful landscape painting in the style of Van Gogh"
image=model(prompt).images[0]
image.show()

import torch
from diffusers import StableDiffusionPipeline
from PIL import Image

#Load stable diffusion Inapinting Model in FP16 (speeds up inference)


device="cuda" if torch.cuda.is_available() else "cpu"
pipe=StableDiffusionInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    torch_dtype=torch.float16 #FP16 precision for speed
).to(device)

#Load the input image and mask image
image=Image.open("images.jpg").convert("RGB") #Load a real world image
mask_image=Image.open("mask1.jpg").convert("L") #black and white mask (white = missing area)

#Define the text prompt


#prompt="a cute cartoon cat"
prompt="a cute cartoon style"

#Run the inpainting model with reduced inference steps
result=pipe(
    prompt=prompt,
    image=image,
    mask_image=mask_image,
    num_inference_steps=30
).images[0]

#save and show the result
result.save("inpainted_image.jpg")
result.show()

print("A-I generated inpainted image saved as " inpainted_image.jpg"")




Q . 12) GRAYSCALE

pip install opencv

import numpy as np
import cv2
import matplotlib.pyplot as plt

net=cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt','colorization_release_v2.caffemodel')
#Load Cluster centers
pts_in_hull=np.load('pts_in_hull.npy',allow_pickle=True)
#Populate cluster centers at 1x1 convolutional kernel
class8=net.getLayerId('class8_ab')
conv8=net.getLayerId('conv8_313_rh')
pts_in_hull = pts_in_hull.transpose().reshape(2,313,1,1)
net.getLayer(class8).blobs=[np.full([1,313], 2.606, np.float32)]

#read the input image
image=cv2.imread('image1.jpg')
#convert the grayscale
gray_image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

#convert to rgb
gray_image_rgb=cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB)

#normalize the image
normalized_image=gray_image_rgb.astype('float32')/255.0
#convert to image to lab
lab_image=cv2.cvtColor(normalized_image,cv2.COLOR_RGB2Lab)

#resize the lightness channel to network input size
resized_l_channel=cv2.resize(lab_image[:,:,0],(224,224))
resized_l_channel-=50

#predict the a and b channel
net.setInput(cv2,dnn.blobFromImage(resized_l_channel))
pred=net.forward()[0,:,:,:].transpose((1,2,0))

#resize the predicted 'ab' image to the same dimension as our input image
pred_resized=cv2.resize(pred,(image.shape[1], image.shape[0]))
#concatenate the original l channel with the predicted 'ab' channels
colorized_image=cv2.cvtColor(colorized_image, cv2.COLOR_Lab2BGR)
#clip any values that fall outside the range[0,1]
colorized_image=(255*colorized_image).astype('uint8')

#save the colorized output as a 3 channel png image
output_filename='colorized_output.png'
cv2.imwrite(output_filename, colorized_image)
print(f'Colorized image saved as {output_filename}')

#show both the original grayscale and colorized image using matplotlib
plt.figure(figsize=(14,7))
#display original grayscale image
plt.subplot(1,2,1)
plt.imshow(gray_image, cmap='gray')
plt.title('Original grayscale image')
plt.axis('off')

#display the colorized image
plt.subplot(1,2,2)
colorized_image_rgb=cv2.cvtColor(colorized_image, cv2.COLOR_BGR2RGB)

plt.imshow(colorized_image_rgb)
plt.title('Colorized image')
plt.axis('off')
plt.show()




Q . 13) SOUND

import torch,torchaudio
from transformers import AutoProcessor , MusicgenForConditionalGeneration

MODEL='facebook/musicgen-small'
processor = AutoProcessor.from_pretrained(MODEL)
model = MusicgenForConditionalGeneration.from_pretrained(MODEL)

#prompt=warm lofi beat with soft piano and vinyl crackle
prompt='India classical music performance with sitar and tabla, gentle rythms and melodic improvisation'
inputs = processor(text=[prompt],return_tensors="pt")
model.generation_config.do_sample=True
model.generation_config.guidance_scale=3.0
model.generation_config.max_new_tokens=50*18

audio=model.generate(**inputs)
sr=model.config.audio_encoder.sampling_rate
torchaudio.save('text_music.wav',audio[0].cpu(),sr)
print('Saved text_music.wav')

# B) Tiny Melody-guided version (condition on your input.wav)
import torch, torchaudio
from transformers import AutoProcessor, MusicgenForConditionalGeneration

MODEL = "facebook/musicgen-small"
processor = AutoProcessor.from_pretrained(MODEL)
model = MusicgenForConditionalGeneration.from_pretrained(MODEL)

prompt = "emotional strings over gentle ambient pads"
guide, sr_in = torchaudio.load("input.wav")   # your short seed (mono/stereo)
sr = model.config.audio_encoder.sampling_rate
if sr_in != sr: guide = torchaudio.functional.resample(guide, sr_in, sr)

inputs = processor(audio=guide, sampling_rate=sr, text=[prompt], return_tensors="pt")
model.generation_config.do_sample = True
model.generation_config.guidance_scale = 3.0
model.generation_config.max_new_tokens = 50*18   # ~18s

audio = model.generate(**inputs)
torchaudio.save("melody_music.wav", audio[0].cpu(), sr)
print("Saved melody_music.wav")














